{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f302446b-5c4c-4deb-9d94-8b608398499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit import ViT\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f903829-1001-4065-959a-1d8df29270d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_official_transformer_params(custom_model_state_dict, official_model_state_dict, dim = 384):\n",
    "    dim = dim\n",
    "    param_mapping = {}\n",
    "    for layer_index in range(12):  # 假设有12层\n",
    "        layer_param_mapping = {\n",
    "            f\"transformer.layers.{layer_index}.0.residual.norm.weight\": f\"Transformer/encoderblock_{layer_index}/LayerNorm_0/scale\",\n",
    "            f\"transformer.layers.{layer_index}.0.residual.norm.bias\": f\"Transformer/encoderblock_{layer_index}/LayerNorm_0/bias\",\n",
    "            # 根据需要添加每一层的其他参数映射...\n",
    "            f\"transformer.layers.{layer_index}.0.residual.norm.weight\":        f\"Transformer/encoderblock_{layer_index}/LayerNorm_0/scale\",\n",
    "            f\"transformer.layers.{layer_index}.0.residual.norm.bias\":          f\"Transformer/encoderblock_{layer_index}/LayerNorm_0/bias\",\n",
    "            f\"transformer.layers.{layer_index}.0.residual.fn.to_q.weight\":     f\"Transformer/encoderblock_{layer_index}/MultiHeadDotProductAttention_1/query/kernel\",\n",
    "            f\"transformer.layers.{layer_index}.0.residual.fn.to_k.weight\":     f\"Transformer/encoderblock_{layer_index}/MultiHeadDotProductAttention_1/key/kernel\",\n",
    "            f\"transformer.layers.{layer_index}.0.residual.fn.to_v.weight\":     f\"Transformer/encoderblock_{layer_index}/MultiHeadDotProductAttention_1/value/kernel\",\n",
    "            f\"transformer.layers.{layer_index}.0.residual.fn.to_out.0.weight\": f\"Transformer/encoderblock_{layer_index}/MultiHeadDotProductAttention_1/out/kernel\",\n",
    "            f\"transformer.layers.{layer_index}.0.residual.fn.to_out.0.bias\":   f\"Transformer/encoderblock_{layer_index}/MultiHeadDotProductAttention_1/out/bias\",\n",
    "            f\"transformer.layers.{layer_index}.1.residual.norm.weight\":        f\"Transformer/encoderblock_{layer_index}/LayerNorm_2/scale\",\n",
    "            f\"transformer.layers.{layer_index}.1.residual.norm.bias\":          f\"Transformer/encoderblock_{layer_index}/LayerNorm_2/bias\",\n",
    "            f\"transformer.layers.{layer_index}.1.residual.fn.FFN1.0.weight\":   f\"Transformer/encoderblock_{layer_index}/MlpBlock_3/Dense_0/kernel\",\n",
    "            f\"transformer.layers.{layer_index}.1.residual.fn.FFN1.0.bias\":     f\"Transformer/encoderblock_{layer_index}/MlpBlock_3/Dense_0/bias\",\n",
    "            f\"transformer.layers.{layer_index}.1.residual.fn.FFN2.0.weight\":   f\"Transformer/encoderblock_{layer_index}/MlpBlock_3/Dense_1/kernel\",\n",
    "            f\"transformer.layers.{layer_index}.1.residual.fn.FFN2.0.bias\":     f\"Transformer/encoderblock_{layer_index}/MlpBlock_3/Dense_1/bias\"\n",
    "        }\n",
    "        # 将当前层的映射合并到总映射中\n",
    "        param_mapping.update(layer_param_mapping)\n",
    "  \n",
    "    # 加载参数\n",
    "    for custom_param, official_param in param_mapping.items():\n",
    "        official_weight = official_model_state_dict[official_param]  \n",
    "        # 根据参数名称选择适当的转换操作\n",
    "        if \"kernel\" in official_param:\n",
    "            if \"FFN\" in custom_param:\n",
    "                # 对于FFN层的权重，进行转置操作\n",
    "                transformed_weight = torch.tensor(official_weight).transpose(0, 1)\n",
    "                custom_model_state_dict[custom_param] = transformed_weight\n",
    "            else:\n",
    "                # 对于MultiHeadDotProductAttention层的权重，进行reshape操作\n",
    "                if \"out\" in custom_param:\n",
    "                    transformed_weight = torch.tensor(official_weight).reshape(-1, dim)\n",
    "                    custom_model_state_dict[custom_param] = transformed_weight\n",
    "                else:\n",
    "                    #对于qkv\n",
    "                    transformed_weight = torch.tensor(official_weight).reshape(dim, -1)\n",
    "                    custom_model_state_dict[custom_param] = transformed_weight\n",
    "        else:\n",
    "            # 对于偏置和LayerNorm的权重，直接使用\n",
    "            transformed_weight = torch.tensor(official_weight)\n",
    "            custom_model_state_dict[custom_param] = transformed_weight\n",
    "            \n",
    "    custom_model_state_dict['patch_embedding.pos_embedding'] = torch.tensor(official_model_state_dict['Transformer/posembed_input/pos_embedding'])\n",
    "    custom_model_state_dict['patch_embedding.cls_token'] = torch.tensor(official_model_state_dict['cls'])\n",
    "    custom_model_state_dict['patch_embedding.patch_to_embedding.bias'] = torch.tensor(official_model_state_dict['embedding/bias'])\n",
    "    custom_model_state_dict['patch_embedding.patch_to_embedding.weight'] = torch.tensor(official_model_state_dict['embedding/kernel']).reshape(-1, dim).transpose(0, 1)\n",
    "            \n",
    "    return custom_model_state_dict\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d2fcaf-d9f9-46bc-8bfc-6a722906ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_small = ViT(\n",
    "            image_size = 224,\n",
    "            patch_size = 16,\n",
    "            num_classes = 100,\n",
    "            dim = 384,\n",
    "            depth = 12,\n",
    "            heads = 6,\n",
    "            mlp_dim = 1536,\n",
    "            dropout = 0.01,\n",
    "            emb_dropout = 0.01)\n",
    "\n",
    "    # 载入预训练权重\n",
    "    pretrained_weights_samll = np.load(r\"../pretrain/augreg_S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz\")\n",
    "\n",
    "    # 调用函数加载参数\n",
    "    custom_model_state_dict = model_small.state_dict()\n",
    "    custom_model_state_dict = load_official_transformer_params(custom_model_state_dict, pretrained_weights_samll)\n",
    "    model_small.load_state_dict(custom_model_state_dict)\n",
    "\n",
    "    torch.save(model_small.state_dict(), r'../pretrain/S_16_model_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e2709-05d8-4e8b-b286-07cd0fb1d590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
